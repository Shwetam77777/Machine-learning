import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

def cost_function(X,w,bias,y):
    N=X.shape[0]
    y_enc = one_hot_encode(y)
    z=net_input(X,w,bias)
    y_pred = softmax(z)
    cost = -(1/N)*np.sum(y_enc*np.log2(y_pred))
    gradient = -(1/N)*np.dot(X.T,(y_enc-y_pred))
    return cost,gradient

def one_hot_encode(y):
    y_enc = pd.get_dummies(y)
    y_enc=np.array(y_enc)
    return y_enc

def net_input(X,weight,bias):
    return np.dot(X,weight)+bias


def softmax(z):
    return (np.exp(z.T)/np.sum(np.exp(z),axis=1)).T

def class_labels(cp):
    return cp.argmax(axis=1)

X=np.array([[5.1,3.5],
          [6.4,3.2],
          [6.9,3.2],
          [5.6,2.8]])

X_test=np.array([[5.2,3.3],
          [6.2,3.1],
          [6.7,3.4],
          [5.1,2.4]])

y = np.array([0,1,2,2])

weight = np.array([[0.1,0.2,0.3],
                   [0.1,0.2,0.3]])

bias = np.array([0.1,0.1,0.1])

z=net_input(X,weight,bias)
cp = softmax(z)
print(class_labels(cp))
#print(np.sum(cp,axis=1))

def gradient_descent():
    i_lst = []
    cost_lst = []
    global X,weight,bias,y
    learning_rate = 0.1
    iterations = 1000
    for i in range(iterations):
        cost,gradient=cost_function(X,weight,bias,y)
        i_lst.append(i)
        cost_lst.append(cost)
        weight = weight-(learning_rate*gradient)

    plt.plot(i_lst,cost_lst)
    plt.show()

gradient_descent()
print(weight)

def accuracy():
    global X_test,weight,bias,y
    z=net_input(X_test,weight,bias)
    cp=softmax(z)
    pred=class_labels(cp)
    print(y==pred)
    return sum(y==pred)/len(y),pred

print(accuracy())